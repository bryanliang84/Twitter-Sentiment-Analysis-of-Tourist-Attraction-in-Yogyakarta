# -*- coding: utf-8 -*-
"""Laporan KP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ThL3WR6IjGal5KzkJ3B4AnkSM2QyUrKQ

## **1. Importing Library**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import requests

"""##**2. Loading dataset**"""

twitter = pd.read_csv('/content/sample_data/twitter-wisata-220221.csv', sep=';', usecols=["id","created", "sentiment", "message"])
twitter.head()

twitter.info()

twitter.dropna(inplace=True)
twitter = twitter[twitter.sentiment != "neutral"]
twitter.reset_index(inplace=True)
twitter

twitter['sentiment'].value_counts()

"""##**3. Data Preprocessing**


"""

import re

def cleaning_text(text):
    #url
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    text =  url_pattern.sub(r'', text)
    #Hashtag
    text = re.sub(r'#', '', text)
    # mention user
    text = re.sub(r'@[\w]*', ' ', text)

    #Simbol lainnya
    punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''
    for x in text.lower():
        if x in punctuations:
            text = text.replace(x, " ")

    # Hapus extra whitespace
    text = text.strip()

    # lowercase
    text = text.lower()
    return text

import re

with open("/content/sample_data/id_stopwords.txt") as f:
    id_stopwords = f.read().splitlines()

with open("/content/sample_data/en_stopwords.txt") as f:
    en_stopwords = f.read().splitlines()

stopwords = id_stopwords + en_stopwords

# Hapus stopwords

import nltk
from nltk import word_tokenize, sent_tokenize
nltk.download('punkt')

def remove_stopword(text, stop_words=stopwords):
    word_tokens = word_tokenize(text)
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    return ' '.join(filtered_sentence)

!pip install Sastrawi

# stemming and lematisasi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stemming_and_lemmatization(text):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    return stemmer.stem(text)

# tokenisasi
def tokenize(text):
    return word_tokenize(text)

# example
text = 'Semalam nonton film ini, paginya ane download, malem langsung nonton.. ane smpai begadang.. hasilnya? Ane Kecewa... http://fb.me/13sZi5lbC'
print(f'Original text: \n{text}\n')

# cleaning text and lowercase
text = cleaning_text(text)
print(f'Cleaned text: \n{text}\n')

# remove stopwords
text = remove_stopword(text)
print(f'Removed stopword: \n{text}\n')

# stemming and lemmatization
text = stemming_and_lemmatization(text)
print(f'Stemmed and lemmatized: \n{text}\n')

# tokenization
text = tokenize(text)
print(f'Tokenized: \n{text}')

def preprocess(text):
    output = cleaning_text(text)

    output = remove_stopword(output)

    output = stemming_and_lemmatization(output)

    output = tokenize(output)

    return output

preprocessed_data = twitter.copy()
preprocessed_data['message'] = twitter['message'].map(preprocess)

preprocessed_data

"""##**4. Feature Extraction**"""

from sklearn.model_selection import train_test_split

X = preprocessed_data['message']
y = preprocessed_data['sentiment']

X.head()

y = y.map({'negative':0, 'positive':1})
y

train_x, test_x, train_y, test_y = train_test_split(X, y,
                                                    test_size=0.2,
                                                    stratify=y,
                                                    random_state=7)

train_x.shape, train_y.shape, test_x.shape, test_y.shape

test=pd.DataFrame(columns = ['message','real sentiment'])
test['message']=test_x
test['real sentiment']=test_y
test

train_x.head()

def build_freqs(tweets, ys):
    yslist = np.squeeze(ys).tolist()

    freqs = {}
    for y, tweet in zip(yslist, tweets):
        for word in tweet:
            pair = (word, y)
            if pair in freqs:
                freqs[pair] += 1
            else:
                freqs[pair] = 1

    return freqs

# dictionary
freqs = build_freqs(train_x.tolist(), train_y.tolist())
print(freqs)
#tipe data
print("type(freqs) = " + str(type(freqs)))
#panjang dictionary
print("len(freqs) = " + str(len(freqs.keys())))

# cek frekuensi
print(f"Frekuensi 'jogja'dengan sentimen positif  : {freqs[('jogja', 1)]}")
print(f"Frekuensi 'jogja' dengan sentimen negatif : {freqs[('jogja', 0)]}")

def extract_features(tweet, freqs):
    x = np.zeros((1, 3))
    x[0,0] = 1

    for word in tweet:

        x[0,1] += freqs.get((word, 1.),0)

        x[0,2] += freqs.get((word, 0.),0)

    assert(x.shape == (1, 3))
    return x

import numpy as np
np.zeros((1, 3))

# test extract features function
tmp = extract_features(train_x.to_numpy()[0], freqs)

print(f'text: {train_x.to_numpy()[0]}')
print(f'feature extraction result: {tmp}')

# test 2
# check for when the words are not in the freqs dictionary
tmp2 = extract_features('jhbdkhfsa shjfbsf sddd', freqs)
print(tmp2)

X_train = np.zeros((len(train_x), 3))
for i in range(len(train_x)):
    X_train[i, :]= extract_features(train_x.to_numpy()[i], freqs)

X_train[:5]

"""##**5. Logistic Regression**"""

def sigmoid(z):
    h = 1 / (1 + np.exp(-z))

    return h

# test sigmoid function
sigmoid(-1.4721)

"""##**6. Training**"""

def gradientDescent(x, y, theta, alpha, num_iters):

    # jumlah baris x
    m = x.shape[0]

    for i in range(0, num_iters):

        # dot product x dan theta
        z = np.dot(x,theta)

        # operasikan dot product x dan theta dalam fungsi sigmoid
        h = sigmoid(z)

        # cost function
        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))

        # update parameter weight theta
        theta = theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))

    J = float(J)
    return J, theta

# Check the function
# Construct a synthetic test case using numpy PRNG functions
np.random.seed(6)
# X input is 10 x 3 with ones for the bias terms
tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)
# Y Labels are 10 x 1
tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)

# Apply gradient descent
tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)
print(f"The cost after training is {tmp_J:.8f}.")
print(f"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}")

Y_train = np.expand_dims(train_y.to_numpy(), axis=1)

init_theta = np.zeros((3, 1))
init_alpha = 1e-4
iter = 2500

J, theta = gradientDescent(X_train, Y_train, init_theta, init_alpha, iter)

print(f"cost        : {J:.8f}.")
print(f"theta akhir : {[round(t, 8) for t in np.squeeze(theta)]}")

"""##**7. Testing**"""

def predict_tweet(tweet, freqs, theta):

    x = extract_features(tweet,freqs)

    y_pred = sigmoid(np.dot(x,theta))

    return y_pred

for tweet in test_x:
    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))

# predict single tweet
tweet1 = 'bagus bagus'
tweet2 = 'jelek jelek'
tweet3 = 'bagus jelek'

print( '%s -> %f' % (tweet1, predict_tweet(preprocess(tweet1), freqs, theta)))
print( '%s -> %f' % (tweet2, predict_tweet(preprocess(tweet2), freqs, theta)))
print( '%s -> %f' % (tweet3, predict_tweet(preprocess(tweet3), freqs, theta)))

"""##**8. Accuracy test**"""

y_hat = []

for tweet in test_x:
    y_pred = predict_tweet(tweet, freqs, theta)
    if y_pred > 0.5:
        y_hat.append(1)
    else:
        y_hat.append(0)
print(y_hat)

len(y_hat)

test['predict sentiment']=y_hat
pd.set_option('display.max_row', None)
test.to_excel('1.xlsx')

test['real sentiment'].value_counts()

test['predict sentiment'].value_counts()

accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)

print(f"Akurasi model regresi logistik = {accuracy:.4f}")